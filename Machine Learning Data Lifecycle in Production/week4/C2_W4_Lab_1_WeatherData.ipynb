{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "C2_W4_Lab_1_WeatherData.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IrnqNnWO04A"
      },
      "source": [
        "# Ungraded Lab: Feature Engineering with Weather Data\n",
        "\n",
        "In this 1st exercise on feature engineering with time series data, you will practice data transformation with the [Weather Dataset](https://www.bgc-jena.mpg.de/wetter/) recorded by the [Max Planck Institute for Biogeochemistry](https://www.bgc-jena.mpg.de/). You will be using [tf.Transform](https://www.tensorflow.org/tfx/transform/get_started) here instead of TFX because, as of this version (1.4), it is more straightforward to preserve the sequence of your records using this framework. If you remember, TFX by default always shuffles the data when ingesting through the `ExampleGen` component and that is not ideal when preparing data for forecasting applications.\n",
        "\n",
        "This dataset has 10-minute intervals of 14 different features such as air temperature, atmospheric pressure, and humidity. For this lab, you will use only the data collected between 2009 and 2016. This section of the dataset was prepared by François Chollet for his book *Deep Learning with Python*.\n",
        "\n",
        "The table below shows the column names, their value formats, and their description.\n",
        "\n",
        "Index| Features      |Format             |Description\n",
        "-----|---------------|-------------------|-----------------------\n",
        "1    |Date Time      |01.01.2009 00:10:00|Date-time reference\n",
        "2    |p (mbar)       |996.52             |The pascal SI derived unit of pressure used to quantify internal pressure. Meteorological reports typically state atmospheric pressure in millibars.\n",
        "3    |T (degC)       |-8.02              |Temperature in Celsius\n",
        "4    |Tpot (K)       |265.4              |Temperature in Kelvin\n",
        "5    |Tdew (degC)    |-8.9               |Temperature in Celsius relative to humidity. Dew Point is a measure of the absolute amount of water in the air, the DP is the temperature at which the air cannot hold all the moisture in it and water condenses.\n",
        "6    |rh (%)         |93.3               |Relative Humidity is a measure of how saturated the air is with water vapor, the %RH determines the amount of water contained within collection objects.\n",
        "7    |VPmax (mbar)   |3.33               |Saturation vapor pressure\n",
        "8    |VPact (mbar)   |3.11               |Vapor pressure\n",
        "9    |VPdef (mbar)   |0.22               |Vapor pressure deficit\n",
        "10   |sh (g/kg)      |1.94               |Specific humidity\n",
        "11   |H2OC (mmol/mol)|3.12               |Water vapor concentration\n",
        "12   |rho (g/m ** 3) |1307.75            |Airtight\n",
        "13   |wv (m/s)       |1.03               |Wind speed\n",
        "14   |max. wv (m/s)  |1.75               |Maximum wind speed\n",
        "15   |wd (deg)       |152.3              |Wind direction in degrees\n",
        "\n",
        "You will perform data preprocessing so that the features can be used to train an LSTM using TensorFlow and Keras downstream. You will not be asked to train a model as the focus is on feature preprocessing.\n",
        "\n",
        "Upon completion, you will have\n",
        "\n",
        "* Explored and visualized the weather time series dataset and declared its schema\n",
        "* Transformed the data for modeling using [tf.Transform](https://www.tensorflow.org/tfx/transform/get_started)\n",
        "* Prepared training dataset windows from the output of `tf.Transform`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDhCGkJoQDEN"
      },
      "source": [
        "## Install Packages\n",
        "\n",
        "First, you will install the `tensorflow_transform` Python package and its dependencies. This includes other modules you will need such as [`apache_beam`](https://beam.apache.org/) for data processing, and [`tfx_bsl`](https://github.com/tensorflow/tfx-bsl) which has utilities for working with Tensorflow data representations.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qONoNgHNWKt3",
        "outputId": "b9fbf65c-420d-4c03-ab23-52b1969ffbec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install tensorflow_transform==1.4.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_transform==1.4.0\n",
            "  Downloading tensorflow_transform-1.4.0-py3-none-any.whl (413 kB)\n",
            "\u001b[K     |████████████████████████████████| 413 kB 8.5 MB/s \n",
            "\u001b[?25hCollecting tfx-bsl<1.5.0,>=1.4.0\n",
            "  Downloading tfx_bsl-1.4.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (19.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.1 MB 441 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<1.20,>=1.16 in /usr/local/lib/python3.7/dist-packages (from tensorflow_transform==1.4.0) (1.19.5)\n",
            "Collecting tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2\n",
            "  Downloading tensorflow-2.6.2-cp37-cp37m-manylinux2010_x86_64.whl (458.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 458.3 MB 10 kB/s \n",
            "\u001b[?25hCollecting apache-beam[gcp]<3,>=2.33\n",
            "  Downloading apache_beam-2.35.0-cp37-cp37m-manylinux2010_x86_64.whl (9.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.9 MB 11.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pydot<2,>=1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow_transform==1.4.0) (1.3.0)\n",
            "Collecting absl-py<0.13,>=0.9\n",
            "  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
            "\u001b[K     |████████████████████████████████| 129 kB 48.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow<6,>=1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_transform==1.4.0) (3.0.0)\n",
            "Collecting tensorflow-metadata<1.5.0,>=1.4.0\n",
            "  Downloading tensorflow_metadata-1.4.0-py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf<4,>=3.13 in /usr/local/lib/python3.7/dist-packages (from tensorflow_transform==1.4.0) (3.17.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py<0.13,>=0.9->tensorflow_transform==1.4.0) (1.15.0)\n",
            "Collecting proto-plus<2,>=1.7.1\n",
            "  Downloading proto_plus-1.19.9-py3-none-any.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 2.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: httplib2<0.20.0,>=0.8 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.33->tensorflow_transform==1.4.0) (0.17.4)\n",
            "Collecting dill<0.3.2,>=0.3.1.1\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 52.1 MB/s \n",
            "\u001b[?25hCollecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading hdfs-2.6.0-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.33->tensorflow_transform==1.4.0) (1.7)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.33->tensorflow_transform==1.4.0) (2.8.2)\n",
            "Requirement already satisfied: grpcio<2,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.33->tensorflow_transform==1.4.0) (1.43.0)\n",
            "Collecting pymongo<4.0.0,>=3.8.0\n",
            "  Downloading pymongo-3.12.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (508 kB)\n",
            "\u001b[K     |████████████████████████████████| 508 kB 63.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.33->tensorflow_transform==1.4.0) (2018.9)\n",
            "Collecting requests<3.0.0,>=2.24.0\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.2 MB/s \n",
            "\u001b[?25hCollecting fastavro<2,>=0.21.4\n",
            "  Downloading fastavro-1.4.9-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 50.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client<5,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.33->tensorflow_transform==1.4.0) (4.1.3)\n",
            "Collecting orjson<4.0\n",
            "  Downloading orjson-3.6.6-cp37-cp37m-manylinux_2_24_x86_64.whl (245 kB)\n",
            "\u001b[K     |████████████████████████████████| 245 kB 69.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions<4,>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.33->tensorflow_transform==1.4.0) (3.10.0.2)\n",
            "Collecting google-cloud-vision<2,>=0.38.0\n",
            "  Downloading google_cloud_vision-1.0.0-py2.py3-none-any.whl (435 kB)\n",
            "\u001b[K     |████████████████████████████████| 435 kB 69.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cachetools<5,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.33->tensorflow_transform==1.4.0) (4.2.4)\n",
            "Collecting google-cloud-language<2,>=1.3.0\n",
            "  Downloading google_cloud_language-1.3.0-py2.py3-none-any.whl (83 kB)\n",
            "\u001b[K     |████████████████████████████████| 83 kB 2.9 MB/s \n",
            "\u001b[?25hCollecting google-cloud-spanner<2,>=1.13.0\n",
            "  Downloading google_cloud_spanner-1.19.1-py2.py3-none-any.whl (255 kB)\n",
            "\u001b[K     |████████████████████████████████| 255 kB 75.1 MB/s \n",
            "\u001b[?25hCollecting google-apitools<0.5.32,>=0.5.31\n",
            "  Downloading google-apitools-0.5.31.tar.gz (173 kB)\n",
            "\u001b[K     |████████████████████████████████| 173 kB 75.7 MB/s \n",
            "\u001b[?25hCollecting google-cloud-recommendations-ai<=0.2.0,>=0.1.0\n",
            "  Downloading google_cloud_recommendations_ai-0.2.0-py2.py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 55.7 MB/s \n",
            "\u001b[?25hCollecting google-cloud-dlp<4,>=3.0.0\n",
            "  Downloading google_cloud_dlp-3.6.0-py2.py3-none-any.whl (111 kB)\n",
            "\u001b[K     |████████████████████████████████| 111 kB 61.7 MB/s \n",
            "\u001b[?25hCollecting grpcio-gcp<1,>=0.2.2\n",
            "  Downloading grpcio_gcp-0.2.2-py2.py3-none-any.whl (9.4 kB)\n",
            "Collecting google-cloud-pubsub<2,>=0.39.0\n",
            "  Downloading google_cloud_pubsub-1.7.0-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 68.0 MB/s \n",
            "\u001b[?25hCollecting google-cloud-bigtable<2,>=0.31.1\n",
            "  Downloading google_cloud_bigtable-1.7.0-py2.py3-none-any.whl (267 kB)\n",
            "\u001b[K     |████████████████████████████████| 267 kB 74.4 MB/s \n",
            "\u001b[?25hCollecting google-cloud-videointelligence<2,>=1.8.0\n",
            "  Downloading google_cloud_videointelligence-1.16.1-py2.py3-none-any.whl (183 kB)\n",
            "\u001b[K     |████████████████████████████████| 183 kB 34.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-bigquery<3,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.33->tensorflow_transform==1.4.0) (1.21.0)\n",
            "Requirement already satisfied: google-cloud-core<2,>=0.28.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.33->tensorflow_transform==1.4.0) (1.0.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.33->tensorflow_transform==1.4.0) (1.35.0)\n",
            "Requirement already satisfied: google-cloud-datastore<2,>=1.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.33->tensorflow_transform==1.4.0) (1.8.0)\n",
            "Collecting google-cloud-bigquery-storage>=2.6.3\n",
            "  Downloading google_cloud_bigquery_storage-2.11.0-py2.py3-none-any.whl (172 kB)\n",
            "\u001b[K     |████████████████████████████████| 172 kB 78.0 MB/s \n",
            "\u001b[?25hCollecting fasteners>=0.14\n",
            "  Downloading fasteners-0.17.3-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.33->tensorflow_transform==1.4.0) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.33->tensorflow_transform==1.4.0) (0.2.8)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.33->tensorflow_transform==1.4.0) (57.4.0)\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<3,>=1.6.0->apache-beam[gcp]<3,>=2.33->tensorflow_transform==1.4.0) (0.4.1)\n",
            "Collecting libcst>=0.2.5\n",
            "  Downloading libcst-0.4.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7 MB 54.3 MB/s \n",
            "\u001b[?25hCollecting google-api-core[grpc]<3.0.0dev,>=1.28.0\n",
            "  Downloading google_api_core-2.4.0-py2.py3-none-any.whl (111 kB)\n",
            "\u001b[K     |████████████████████████████████| 111 kB 71.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.28.0->google-cloud-bigquery-storage>=2.6.3->apache-beam[gcp]<3,>=2.33->tensorflow_transform==1.4.0) (1.54.0)\n",
            "Collecting grpcio-status<2.0dev,>=1.33.2\n",
            "  Downloading grpcio_status-1.43.0-py3-none-any.whl (10.0 kB)\n",
            "Collecting google-cloud-bigtable<2,>=0.31.1\n",
            "  Downloading google_cloud_bigtable-1.6.1-py2.py3-none-any.whl (267 kB)\n",
            "\u001b[K     |████████████████████████████████| 267 kB 69.6 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_bigtable-1.6.0-py2.py3-none-any.whl (267 kB)\n",
            "\u001b[K     |████████████████████████████████| 267 kB 79.5 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_bigtable-1.5.1-py2.py3-none-any.whl (266 kB)\n",
            "\u001b[K     |████████████████████████████████| 266 kB 66.9 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_bigtable-1.5.0-py2.py3-none-any.whl (266 kB)\n",
            "\u001b[K     |████████████████████████████████| 266 kB 76.0 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_bigtable-1.4.0-py2.py3-none-any.whl (265 kB)\n",
            "\u001b[K     |████████████████████████████████| 265 kB 78.1 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_bigtable-1.3.0-py2.py3-none-any.whl (259 kB)\n",
            "\u001b[K     |████████████████████████████████| 259 kB 75.1 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_bigtable-1.2.1-py2.py3-none-any.whl (234 kB)\n",
            "\u001b[K     |████████████████████████████████| 234 kB 76.8 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_bigtable-1.2.0-py2.py3-none-any.whl (234 kB)\n",
            "\u001b[K     |████████████████████████████████| 234 kB 81.0 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_bigtable-1.1.0-py2.py3-none-any.whl (234 kB)\n",
            "\u001b[K     |████████████████████████████████| 234 kB 73.1 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_bigtable-1.0.0-py2.py3-none-any.whl (232 kB)\n",
            "\u001b[K     |████████████████████████████████| 232 kB 80.2 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_bigtable-0.34.0-py2.py3-none-any.whl (232 kB)\n",
            "\u001b[K     |████████████████████████████████| 232 kB 79.8 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_bigtable-0.33.0-py2.py3-none-any.whl (230 kB)\n",
            "\u001b[K     |████████████████████████████████| 230 kB 71.5 MB/s \n",
            "\u001b[?25hCollecting grpc-google-iam-v1<0.12dev,>=0.11.4\n",
            "  Downloading grpc-google-iam-v1-0.11.4.tar.gz (12 kB)\n",
            "Collecting google-cloud-bigtable<2,>=0.31.1\n",
            "  Downloading google_cloud_bigtable-0.32.2-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 66.5 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_bigtable-0.32.1-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 54.2 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_bigtable-0.32.0-py2.py3-none-any.whl (155 kB)\n",
            "\u001b[K     |████████████████████████████████| 155 kB 56.8 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_bigtable-0.31.1-py2.py3-none-any.whl (154 kB)\n",
            "\u001b[K     |████████████████████████████████| 154 kB 62.3 MB/s \n",
            "\u001b[?25hINFO: pip is looking at multiple versions of google-api-core to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of google-api-core[grpc] to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting google-api-core[grpc]<3.0.0dev,>=1.28.0\n",
            "  Downloading google_api_core-2.3.2-py2.py3-none-any.whl (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 60.4 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-2.3.0-py2.py3-none-any.whl (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 63.4 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-2.2.2-py2.py3-none-any.whl (95 kB)\n",
            "\u001b[K     |████████████████████████████████| 95 kB 6.2 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-2.2.1-py2.py3-none-any.whl (95 kB)\n",
            "\u001b[K     |████████████████████████████████| 95 kB 5.6 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-2.2.0-py2.py3-none-any.whl (95 kB)\n",
            "\u001b[K     |████████████████████████████████| 95 kB 4.2 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-2.1.1-py2.py3-none-any.whl (95 kB)\n",
            "\u001b[K     |████████████████████████████████| 95 kB 4.6 MB/s \n",
            "\u001b[?25h  Downloading google_api_core-2.1.0-py2.py3-none-any.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 4.9 MB/s \n",
            "\u001b[?25hINFO: pip is looking at multiple versions of google-api-core to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of google-api-core[grpc] to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading google_api_core-2.0.1-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 647 kB/s \n",
            "\u001b[?25h  Downloading google_api_core-2.0.0-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 511 kB/s \n",
            "\u001b[?25h  Downloading google_api_core-1.31.5-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.28.0->google-cloud-bigquery-storage>=2.6.3->apache-beam[gcp]<3,>=2.33->tensorflow_transform==1.4.0) (21.3)\n",
            "Collecting google-cloud-core<2,>=0.28.1\n",
            "  Downloading google_cloud_core-1.7.2-py2.py3-none-any.whl (28 kB)\n",
            "Collecting grpc-google-iam-v1<0.13dev,>=0.12.3\n",
            "  Downloading grpc-google-iam-v1-0.12.3.tar.gz (13 kB)\n",
            "Collecting pytz>=2018.3\n",
            "  Downloading pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 69.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.33->tensorflow_transform==1.4.0) (0.6.2)\n",
            "Collecting typing-inspect>=0.4.0\n",
            "  Downloading typing_inspect-0.7.1-py3-none-any.whl (8.4 kB)\n",
            "Collecting pyyaml>=5.2\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 72.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5,>=2.0.1->apache-beam[gcp]<3,>=2.33->tensorflow_transform==1.4.0) (0.4.8)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core[grpc]<3.0.0dev,>=1.28.0->google-cloud-bigquery-storage>=2.6.3->apache-beam[gcp]<3,>=2.33->tensorflow_transform==1.4.0) (3.0.7)\n",
            "Collecting protobuf<4,>=3.13\n",
            "  Downloading protobuf-3.19.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 51.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.33->tensorflow_transform==1.4.0) (2.0.10)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.33->tensorflow_transform==1.4.0) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.33->tensorflow_transform==1.4.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.33->tensorflow_transform==1.4.0) (2021.10.8)\n",
            "Collecting clang~=5.0\n",
            "  Downloading clang-5.0.tar.gz (30 kB)\n",
            "Collecting tensorboard<2.7,>=2.6.0\n",
            "  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 40.8 MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.7,>=2.6.0\n",
            "  Downloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 73.3 MB/s \n",
            "\u001b[?25hCollecting typing-extensions<4,>=3.7.0\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Collecting flatbuffers~=1.12.0\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Collecting keras<2.7,>=2.6.0\n",
            "  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 55.3 MB/s \n",
            "\u001b[?25hCollecting wrapt~=1.12.1\n",
            "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow_transform==1.4.0) (1.6.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow_transform==1.4.0) (0.37.1)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow_transform==1.4.0) (3.1.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow_transform==1.4.0) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow_transform==1.4.0) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow_transform==1.4.0) (3.3.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow_transform==1.4.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow_transform==1.4.0) (0.2.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow_transform==1.4.0) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.7,>=2.6.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow_transform==1.4.0) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.7,>=2.6.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow_transform==1.4.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.7,>=2.6.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow_transform==1.4.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.7,>=2.6.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow_transform==1.4.0) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.7,>=2.6.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow_transform==1.4.0) (0.6.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow_transform==1.4.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow_transform==1.4.0) (4.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow_transform==1.4.0) (3.7.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow_transform==1.4.0) (3.1.1)\n",
            "Collecting tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<3,>=1.15\n",
            "  Downloading tensorflow_serving_api-2.7.0-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: google-api-python-client<2,>=1.7.11 in /usr/local/lib/python3.7/dist-packages (from tfx-bsl<1.5.0,>=1.4.0->tensorflow_transform==1.4.0) (1.12.10)\n",
            "Requirement already satisfied: pandas<2,>=1.0 in /usr/local/lib/python3.7/dist-packages (from tfx-bsl<1.5.0,>=1.4.0->tensorflow_transform==1.4.0) (1.1.5)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.11->tfx-bsl<1.5.0,>=1.4.0->tensorflow_transform==1.4.0) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.11->tfx-bsl<1.5.0,>=1.4.0->tensorflow_transform==1.4.0) (0.0.4)\n",
            "  Downloading tensorflow_serving_api-2.6.2-py2.py3-none-any.whl (37 kB)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Building wheels for collected packages: dill, google-apitools, grpc-google-iam-v1, clang, wrapt\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78544 sha256=7869b4818033c9a449ab680ea26777e54c8155b7b9978c42eafc993b9ab3f52b\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n",
            "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-apitools: filename=google_apitools-0.5.31-py3-none-any.whl size=131039 sha256=fb60f0e0cd85edf631b20652978486b9b14444aa40d30c88bcc7d8de531ffc72\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/b5/2f/1cc3cf2b31e7a9cd1508731212526d9550271274d351c96f16\n",
            "  Building wheel for grpc-google-iam-v1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grpc-google-iam-v1: filename=grpc_google_iam_v1-0.12.3-py3-none-any.whl size=18515 sha256=91381814549ed2afb1345e45f62b3f42776f574d3bf3d3ddfbe57a22263228f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/ee/67/2e444183030cb8d31ce8b34cee34a7afdbd3ba5959ea846380\n",
            "  Building wheel for clang (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30694 sha256=38695727fe7b04099e4e3692ded356e28c173dee87a553677c20b3419f4064ce\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/91/04/971b4c587cf47ae952b108949b46926f426c02832d120a082a\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=68711 sha256=4f295c264facbb3b5d207c3635828c77ad9b4f9355808cfb6d8aa9b68dcc92ec\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
            "Successfully built dill google-apitools grpc-google-iam-v1 clang wrapt\n",
            "Installing collected packages: typing-extensions, requests, protobuf, pytz, mypy-extensions, typing-inspect, pyyaml, grpcio-gcp, google-api-core, absl-py, wrapt, tensorflow-estimator, tensorboard, pymongo, proto-plus, orjson, libcst, keras, hdfs, grpc-google-iam-v1, google-cloud-core, flatbuffers, fasteners, fastavro, dill, clang, tensorflow, google-cloud-vision, google-cloud-videointelligence, google-cloud-spanner, google-cloud-recommendations-ai, google-cloud-pubsub, google-cloud-language, google-cloud-dlp, google-cloud-bigtable, google-cloud-bigquery-storage, google-apitools, apache-beam, tensorflow-serving-api, tensorflow-metadata, tfx-bsl, tensorflow-transform\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 3.10.0.2\n",
            "    Uninstalling typing-extensions-3.10.0.2:\n",
            "      Successfully uninstalled typing-extensions-3.10.0.2\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.17.3\n",
            "    Uninstalling protobuf-3.17.3:\n",
            "      Successfully uninstalled protobuf-3.17.3\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2018.9\n",
            "    Uninstalling pytz-2018.9:\n",
            "      Successfully uninstalled pytz-2018.9\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: google-api-core\n",
            "    Found existing installation: google-api-core 1.26.3\n",
            "    Uninstalling google-api-core-1.26.3:\n",
            "      Successfully uninstalled google-api-core-1.26.3\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.0.0\n",
            "    Uninstalling absl-py-1.0.0:\n",
            "      Successfully uninstalled absl-py-1.0.0\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.13.3\n",
            "    Uninstalling wrapt-1.13.3:\n",
            "      Successfully uninstalled wrapt-1.13.3\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: pymongo\n",
            "    Found existing installation: pymongo 4.0.1\n",
            "    Uninstalling pymongo-4.0.1:\n",
            "      Successfully uninstalled pymongo-4.0.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.7.0\n",
            "    Uninstalling keras-2.7.0:\n",
            "      Successfully uninstalled keras-2.7.0\n",
            "  Attempting uninstall: google-cloud-core\n",
            "    Found existing installation: google-cloud-core 1.0.3\n",
            "    Uninstalling google-cloud-core-1.0.3:\n",
            "      Successfully uninstalled google-cloud-core-1.0.3\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0\n",
            "    Uninstalling flatbuffers-2.0:\n",
            "      Successfully uninstalled flatbuffers-2.0\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.4\n",
            "    Uninstalling dill-0.3.4:\n",
            "      Successfully uninstalled dill-0.3.4\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "  Attempting uninstall: google-cloud-language\n",
            "    Found existing installation: google-cloud-language 1.2.0\n",
            "    Uninstalling google-cloud-language-1.2.0:\n",
            "      Successfully uninstalled google-cloud-language-1.2.0\n",
            "  Attempting uninstall: google-cloud-bigquery-storage\n",
            "    Found existing installation: google-cloud-bigquery-storage 1.1.0\n",
            "    Uninstalling google-cloud-bigquery-storage-1.1.0:\n",
            "      Successfully uninstalled google-cloud-bigquery-storage-1.1.0\n",
            "  Attempting uninstall: tensorflow-metadata\n",
            "    Found existing installation: tensorflow-metadata 1.6.0\n",
            "    Uninstalling tensorflow-metadata-1.6.0:\n",
            "      Successfully uninstalled tensorflow-metadata-1.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "multiprocess 0.70.12.2 requires dill>=0.3.4, but you have dill 0.3.1.1 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed absl-py-0.12.0 apache-beam-2.35.0 clang-5.0 dill-0.3.1.1 fastavro-1.4.9 fasteners-0.17.3 flatbuffers-1.12 google-api-core-1.31.5 google-apitools-0.5.31 google-cloud-bigquery-storage-2.11.0 google-cloud-bigtable-1.7.0 google-cloud-core-1.7.2 google-cloud-dlp-3.6.0 google-cloud-language-1.3.0 google-cloud-pubsub-1.7.0 google-cloud-recommendations-ai-0.2.0 google-cloud-spanner-1.19.1 google-cloud-videointelligence-1.16.1 google-cloud-vision-1.0.0 grpc-google-iam-v1-0.12.3 grpcio-gcp-0.2.2 hdfs-2.6.0 keras-2.6.0 libcst-0.4.1 mypy-extensions-0.4.3 orjson-3.6.6 proto-plus-1.19.9 protobuf-3.19.4 pymongo-3.12.3 pytz-2021.3 pyyaml-6.0 requests-2.27.1 tensorboard-2.6.0 tensorflow-2.6.2 tensorflow-estimator-2.6.0 tensorflow-metadata-1.4.0 tensorflow-serving-api-2.6.2 tensorflow-transform-1.4.0 tfx-bsl-1.4.0 typing-extensions-3.7.4.3 typing-inspect-0.7.1 wrapt-1.12.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "pytz"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install six==1.10"
      ],
      "metadata": {
        "id": "ssLKyizfwoSM",
        "outputId": "826c87f3-1597-4a45-e30f-6a9e5fe72902",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting six==1.10\n",
            "  Downloading six-1.10.0-py2.py3-none-any.whl (10 kB)\n",
            "Installing collected packages: six\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.6.2 requires six~=1.15.0, but you have six 1.10.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.10.0 which is incompatible.\n",
            "google-cloud-core 1.7.2 requires six>=1.12.0, but you have six 1.10.0 which is incompatible.\n",
            "google-apitools 0.5.31 requires six>=1.12.0, but you have six 1.10.0 which is incompatible.\n",
            "google-api-python-client 1.12.10 requires six<2dev,>=1.13.0, but you have six 1.10.0 which is incompatible.\n",
            "google-api-core 1.31.5 requires six>=1.13.0, but you have six 1.10.0 which is incompatible.\n",
            "dm-tree 0.1.6 requires six>=1.12.0, but you have six 1.10.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed six-1.10.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvoAuLGMQXhL"
      },
      "source": [
        "*Note: In Google Colab, you need to restart the runtime at this point to finalize updating the packages you just installed. You can do so by clicking the `Restart Runtime` at the end of the output cell above (after installation), or by selecting `Runtime > Restart Runtime` in the Menu bar. **Please do not proceed to the next section without restarting.** You can also ignore the errors about version incompatibility of some of the bundled packages because we won't be using those in this notebook.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyHZtPotQhL0"
      },
      "source": [
        "## Imports\n",
        "\n",
        "Running the imports below should not show any error. Otherwise, please restart your runtime or re-run the package installation cell above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5isfzOcWbCl",
        "outputId": "065e0cce-0b39-4a88-ff15-c4f559b56dcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "source": [
        "import apache_beam as beam\n",
        "print('Apache Beam version: {}'.format(beam.__version__))\n",
        "\n",
        "import tensorflow as tf\n",
        "print('Tensorflow version: {}'.format(tf.__version__))\n",
        "\n",
        "import tensorflow_transform as tft\n",
        "from tensorflow_transform import beam as tft_beam\n",
        "from tensorflow_transform.tf_metadata import dataset_metadata\n",
        "from tensorflow_transform.tf_metadata import schema_utils\n",
        "print('TensorFlow Transform version: {}'.format(tft.__version__))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnknownExtra",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mrequires\u001b[0;34m(self, extras)\u001b[0m\n\u001b[1;32m   2738\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2739\u001b[0;31m                 \u001b[0mdeps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msafe_extra\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2740\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'grpc'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mUnknownExtra\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-3c23bee680c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mapache_beam\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Apache Beam version: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensorflow version: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/apache_beam/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mapache_beam\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcoders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mapache_beam\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mapache_beam\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypehints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mapache_beam\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/apache_beam/io/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mapache_beam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigquery\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mapache_beam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpubsub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mapache_beam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgcsio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/apache_beam/io/gcp/pubsub.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpubsub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0mpubsub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/cloud/pubsub.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpubsub_v1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPublisherClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpubsub_v1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSubscriberClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpubsub_v1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/cloud/pubsub_v1/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpubsub_v1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpubsub_v1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpublisher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpubsub_v1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msubscriber\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/cloud/pubsub_v1/publisher/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpubsub_v1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublisher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/cloud/pubsub_v1/publisher/client.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpubsub_v1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_gapic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpubsub_v1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpubsub_v1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgapic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpublisher_client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpubsub_v1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgapic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransports\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpublisher_grpc_transport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpubsub_v1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublisher\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/cloud/pubsub_v1/gapic/publisher_client.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0m_GAPIC_LIBRARY_VERSION\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"google-cloud-pubsub\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mget_distribution\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_provider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected string, Requirement, or Distribution\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mget_provider\u001b[0;34m(moduleOrReq)\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;34m\"\"\"Return an IResourceProvider for the named module or requirement\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mworking_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mrequire\u001b[0;34m(self, *requirements)\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0mincluded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meven\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mwere\u001b[0m \u001b[0malready\u001b[0m \u001b[0mactivated\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mworking\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m         \"\"\"\n\u001b[0;32m--> 886\u001b[0;31m         \u001b[0mneeded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_requirements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequirements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mneeded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(self, requirements, env, installer, replace_conflicting, extras)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# push the new requirements onto the stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m             \u001b[0mnew_requirements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m             \u001b[0mrequirements\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_requirements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mrequires\u001b[0;34m(self, extras)\u001b[0m\n\u001b[1;32m   2741\u001b[0m                 raise UnknownExtra(\n\u001b[1;32m   2742\u001b[0m                     \u001b[0;34m\"%s has no such extra feature %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2743\u001b[0;31m                 ) from e\n\u001b[0m\u001b[1;32m   2744\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdeps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnknownExtra\u001b[0m: google-api-core 1.26.3 has no such extra feature 'grpc'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll_BqMx4QyP5"
      },
      "source": [
        "## Download the Data\n",
        "\n",
        "Next, you will download the data and put it in your workspace."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu4uJ4RgEfE8"
      },
      "source": [
        "import os\n",
        "\n",
        "# Directory of the raw data files\n",
        "DATA_DIR = '/content/data/'\n",
        "\n",
        "# Download the dataset\n",
        "!wget -nc https://raw.githubusercontent.com/https-deeplearning-ai/MLEP-public/main/course2/week4-ungraded-lab/data/jena_climate_2009_2016.csv -P {DATA_DIR}\n",
        "\n",
        "# Assign data path to a variable for easy reference\n",
        "INPUT_FILE = os.path.join(DATA_DIR, 'jena_climate_2009_2016.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKDJaZfEQ1yT"
      },
      "source": [
        "You can now preview the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ohkBeclndhL"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Put dataset in a dataframe\n",
        "df = pd.read_csv(INPUT_FILE, header=0, index_col=0)\n",
        "\n",
        "# Preview the last few rows\n",
        "df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poxo5wBt7oI7"
      },
      "source": [
        "As you can see above, an observation is recorded every 10 minutes. This means that, for a single hour, you will have 6 observations. Similarly, a single day will contain 144 (6x24) observations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7-nvQoPgJ32"
      },
      "source": [
        "##Inspect the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yRWsz06NdhF"
      },
      "source": [
        "You can then inspect the data to see if there are any issues that you need to address before feeding it to your model. First, you will generate some descriptive statistics using the [`describe()`](https://www.geeksforgeeks.org/python-pandas-dataframe-describe-method/) method of the dataframe.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvF4w5rGNUnf"
      },
      "source": [
        "df.describe().transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmOcUJXReqhg"
      },
      "source": [
        "You can see that the min value for `wv (m/s)` and `max. wv(m/s)` is `-9999.0`. Those are pretty intense winds and based on the other data points, these just look like faulty measurements. This is more pronounced when you visualize the data using the utilities below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QAfG_lJ__JK"
      },
      "source": [
        "# Define feature keys\n",
        "\n",
        "TIMESTAMP_FEATURES = [\"Date Time\"]\n",
        "NUMERIC_FEATURES = [\n",
        "    \"p (mbar)\",\n",
        "    \"T (degC)\",\n",
        "    \"Tpot (K)\",\n",
        "    \"Tdew (degC)\", \n",
        "    \"rh (%)\", \n",
        "    \"VPmax (mbar)\", \n",
        "    \"VPact (mbar)\", \n",
        "    \"VPdef (mbar)\", \n",
        "    \"sh (g/kg)\",\n",
        "    \"H2OC (mmol/mol)\",\n",
        "    \"rho (g/m**3)\",\n",
        "    \"wv (m/s)\",\n",
        "    \"max. wv (m/s)\",\n",
        "    \"wd (deg)\",\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egnSszZ6Ia6k"
      },
      "source": [
        "#@ title Visualization Utilities\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Color Palette\n",
        "colors = [\n",
        "    \"blue\",\n",
        "    \"orange\",\n",
        "    \"green\",\n",
        "    \"red\",\n",
        "    \"purple\",\n",
        "    \"brown\",\n",
        "    \"pink\",\n",
        "    \"gray\",\n",
        "    \"olive\",\n",
        "    \"cyan\",\n",
        "]\n",
        "\n",
        "# Plots each column as a time series\n",
        "def visualize_plots(dataset, columns):\n",
        "    features = dataset[columns]\n",
        "    fig, axes = plt.subplots(\n",
        "        nrows=len(columns)//2 + len(columns)%2, ncols=2, figsize=(15, 20), dpi=80, facecolor=\"w\", edgecolor=\"k\"\n",
        "    )\n",
        "    for i, col in enumerate(columns):\n",
        "        c = colors[i % (len(colors))]\n",
        "        t_data = dataset[col]\n",
        "        t_data.index = dataset.index\n",
        "        t_data.head()\n",
        "        ax = t_data.plot(\n",
        "            ax=axes[i // 2, i % 2],\n",
        "            color=c,\n",
        "            title=\"{}\".format(col),\n",
        "            rot=25,\n",
        "        )\n",
        "    ax.legend([col])\n",
        "    plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLa8UAe3JDZ3"
      },
      "source": [
        "# Visualize the dataset\n",
        "visualize_plots(df, NUMERIC_FEATURES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHdr5yhmUxVy"
      },
      "source": [
        "As you can see, there's a very big downward spike towards `-9999` for the two features mentioned. You may know of different methods in handling outliers but for simplicity in this exercise, you will just set these to `0`. You can visualize the plots again after making this change."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zP5RvoFU7KB"
      },
      "source": [
        "# Set the wind velocity outliers to 0\n",
        "wv = df['wv (m/s)']\n",
        "bad_wv = wv == -9999.0\n",
        "wv[bad_wv] = 0.0\n",
        "\n",
        "# Set the max wind velocity outliers to 0\n",
        "max_wv = df['max. wv (m/s)']\n",
        "bad_max_wv = max_wv == -9999.0\n",
        "max_wv[bad_max_wv] = 0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXkjakrEVEYn"
      },
      "source": [
        "visualize_plots(df, NUMERIC_FEATURES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4MXx8JWapGY"
      },
      "source": [
        "Take note that you are just visualizing the Pandas dataframe here. You will do this simple data cleaning again later when Tensorflow Transform consumes the raw CSV file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "730ghIrjbXhu"
      },
      "source": [
        "## Feature Engineering\n",
        "\n",
        "Now you will be doing feature engineering. There are several things to note before doing the transformation:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VtsR_d2hhNH"
      },
      "source": [
        "\n",
        "\n",
        "### Correlated features\n",
        "\n",
        "You may want to drop redundant features to reduce the complexity of your model. Let's see what features are highly correlated with each other by plotting the correlation matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sejFrBtwhjV_"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "def show_correlation_heatmap(dataframe):\n",
        "    plt.figure(figsize=(20,20))\n",
        "    cor = dataframe.corr()\n",
        "    sns.heatmap(cor, annot=True, cmap=plt.cm.PuBu)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7uReJtJhqyM"
      },
      "source": [
        "show_correlation_heatmap(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsMMBBm8haJS"
      },
      "source": [
        "You can observe that `Tpot (K)`, `Tdew (degC)`, `VPmax(mbar)`, `Vpact(mbar)`, `VPdef (mbar)`, `sh(g/kg)` and `H2OC` are highly positively correlated to the target `T (degC)`. Likewise, `rho` is highly negatively correlated to the target. \n",
        "\n",
        "In the features that are positively correlated, you can see that `VPmax (mbar)` is highly correlated to some features like `Vpact (mbar)`, `Tdew (degC)` and `Tpot (K)`. Hence, for the sake of this exercise you can drop these features and retain `VPmax (mbar)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63D0d5nhjbQ5"
      },
      "source": [
        "# Features to filter out\n",
        "FEATURES_TO_REMOVE = [\"Tpot (K)\", \"Tdew (degC)\",\"VPact (mbar)\" , \"H2OC (mmol/mol)\", \"max. wv (m/s)\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GFwevJxkqyY"
      },
      "source": [
        "### Distribution of Wind Data\n",
        "\n",
        "The last column of the data, `wd (deg)`, gives the wind direction in units of degrees. However, angles in this current format do not make good model inputs. 360° and 0° should be close to each other and wrap around smoothly. Direction shouldn't matter if the wind is not blowing. This will be easier for the model to interpret if you convert the wind direction and velocity columns to a wind vector. Observe how sine and cosine are used to generate wind vector features (`Wx` and `Wy`) in the `preprocessing_fn()` later.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfiFpkMRNduM"
      },
      "source": [
        "### Date Time Feature\n",
        "\n",
        "Dealing with weather, you can expect patterns depending on when the measurements are made. For example, temperatures are generally colder at night and wind velocities might be higher during typhoon season. Thus, the `Date Time` column is a good input to your model to take daily and yearly periodicity into account. \n",
        "\n",
        "To do this, you will first use the [datetime](https://docs.python.org/3/library/datetime.htm) Python module to convert the current date time string format (i.e. _day.month.year hour:minute:second_) to a timestamp with units in seconds. Then, a simple approach to generate a periodic signal is to again use sine and cosine to convert the timestamp to clear \"Time of day\" (`Day sin`, `Day cos`) and \"Time of year\" (`Year sin`, `Year cos`) signals. You will see these conversions in the `clean_fn()` and `preprocessing_fn()`. \n",
        "\n",
        "You can see the `clean_fn()` utility function below that removes wind velocity outliers and converts the date time string to a [Unix timestamp](https://www.unixtimestamp.com/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Lv_foXhGpFD"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# combine features into one list\n",
        "ordered_columns = TIMESTAMP_FEATURES + NUMERIC_FEATURES\n",
        "\n",
        "# index of the date time string\n",
        "date_time_idx = ordered_columns.index(TIMESTAMP_FEATURES[0])\n",
        "\n",
        "# index of the 'wv (m/s)' feature\n",
        "wv_idx = ordered_columns.index('wv (m/s)')\n",
        "\n",
        "def clean_fn(line):\n",
        "  '''\n",
        "  Converts datetime strings in the CSV to Unix timestamps and removes outliers\n",
        "  the wind velocity column. Used as part of\n",
        "  the transform pipeline.\n",
        "\n",
        "  Args:\n",
        "    line (string) - one row of a CSV file\n",
        "  \n",
        "  Returns:\n",
        "\n",
        "  '''\n",
        "\n",
        "  # Split the CSV string to a list\n",
        "  line_split = line.split(b',')\n",
        "\n",
        "  # Decodes the timestamp string to utf-8\n",
        "  date_time_string = line_split[date_time_idx].decode(\"utf-8\")\n",
        "\n",
        "  # Creates a datetime object from the timestamp string\n",
        "  date_time = datetime.strptime(date_time_string, '%d.%m.%Y %H:%M:%S')\n",
        "\n",
        "  # Generates a timestamp from the object\n",
        "  timestamp = datetime.timestamp(date_time)\n",
        "\n",
        "  # Overwrites the string timestamp in the row with the timestamp in seconds\n",
        "  line_split[date_time_idx] = bytes(str(timestamp), 'utf-8')\n",
        "\n",
        "  # Check if wind velocity is an outlier\n",
        "  if line_split[wv_idx] == b'-9999.0':\n",
        "\n",
        "    # Overwrite with default value of 0\n",
        "    line_split[wv_idx] = b'0.0'\n",
        "\n",
        "  # rejoin the list item into one string\n",
        "  mod_line = b','.join(line_split)\n",
        "\n",
        "  return mod_line"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KroNS27tp-Eu"
      },
      "source": [
        "### Create a `tf.Transform` preprocessing_fn\n",
        "\n",
        "With the considerations above, you can now declare your `preprocessing_fn()`. This will be used by Tensorflow Transform to create a transformation graph that will preprocess model inputs. In a nutshell, your preprocessing function will perform the following steps:\n",
        "\n",
        "1. Perform feature selection by deleting the unwanted features. \n",
        "2. Transform wind direction and velocity columns into a wind vector.\n",
        "3. Convert date in timestamp to a usable signal by using `sin` and `cos` to convert the time to clear \"Time of day\" and \"Time of year\" signals. \n",
        "4. Normalize the float features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neDQbbY30Sdu"
      },
      "source": [
        "import numpy as np\n",
        "import math as m\n",
        "\n",
        "def preprocessing_fn(inputs):\n",
        "  \"\"\"Preprocess input columns into transformed columns.\"\"\"\n",
        "  \n",
        "  outputs = inputs.copy()\n",
        "\n",
        "  # Filter redundant features\n",
        "  for key in FEATURES_TO_REMOVE:\n",
        "    del outputs[key]\n",
        "\n",
        "  # Convert degrees to radians\n",
        "  pi = tf.constant(m.pi)\n",
        "  wd_rad = inputs['wd (deg)'] * pi / 180.0\n",
        "\n",
        "  # Calculate the wind x and y components.\n",
        "  outputs['Wx'] = inputs['wv (m/s)'] * tf.math.cos(wd_rad)\n",
        "  outputs['Wy'] = inputs['wv (m/s)'] * tf.math.sin(wd_rad)\n",
        "\n",
        "  # Delete `wv (m/s)` and `wd (deg)` after getting the wind vector\n",
        "  del outputs['wv (m/s)']\n",
        "  del outputs['wd (deg)']\n",
        "\n",
        "  # Get day and year in seconds\n",
        "  day = tf.cast(24*60*60, tf.float32)\n",
        "  year = tf.cast((365.2425)*day, tf.float32)\n",
        "\n",
        "  # Get timestamp feature\n",
        "  timestamp_s = outputs['Date Time']\n",
        "\n",
        "  # Convert timestamps into periodic signals\n",
        "  outputs['Day sin'] = tf.math.sin(timestamp_s * (2 * pi / day))\n",
        "  outputs['Day cos'] = tf.math.cos(timestamp_s * (2 * pi / day))\n",
        "  outputs['Year sin'] = tf.math.sin(timestamp_s * (2 * pi / year))\n",
        "  outputs['Year cos'] = tf.math.cos(timestamp_s * (2 * pi / year))\n",
        "\n",
        "  # Delete timestamp feature\n",
        "  del outputs['Date Time']\n",
        "\n",
        "  # Declare final list of features\n",
        "  FINAL_FEATURE_LIST =  [\"p (mbar)\",\n",
        "    \"T (degC)\",\n",
        "    \"rh (%)\", \n",
        "    \"VPmax (mbar)\", \n",
        "    \"VPdef (mbar)\", \n",
        "    \"sh (g/kg)\",\n",
        "    \"rho (g/m**3)\",\n",
        "    \"Wx\",\n",
        "    \"Wy\",\n",
        "    \"Day sin\",\n",
        "    'Day cos',\n",
        "    'Year sin',\n",
        "    'Year cos'\n",
        "    ]\n",
        "\n",
        "  # Scale all features\n",
        "  for key in FINAL_FEATURE_LIST:\n",
        "    outputs[key] = tft.scale_to_0_1(outputs[key])\n",
        "\n",
        "  return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SSRMC37SkyI"
      },
      "source": [
        "## Transform the data\n",
        "\n",
        "You're almost ready to start transforming the data in an Apache Beam pipeline. Before doing so, you will declare just a few more utility functions and variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmEOKqhno-pC"
      },
      "source": [
        "### Train Test Split\n",
        "\n",
        "First, you will define how your dataset will be split. You will use the first 300,000 observations for training and the rest for testing.\n",
        "\n",
        "You will extract the date time object of the 300,000th observation to use it in partitioning the dataset using [`Beam.Partition()`](https://beam.apache.org/documentation/transforms/python/elementwise/partition/). This method expects a `partition_fn()` that returns an integer indicating the partition number. Since you will only need two (i.e. train and test), you will make the function return `0` when it is part of the train split, and `1` for the test. See how this is implemented in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PJzIMFgBl-b"
      },
      "source": [
        "# Number of records to include in the train split\n",
        "TRAIN_SPLIT = 300000\n",
        "\n",
        "# Get date time of the last element in the train split\n",
        "date_time_train_boundary = df.iloc[TRAIN_SPLIT - 1].name\n",
        "\n",
        "# Creates a datetime object from the timestamp string\n",
        "date_time_train_boundary = datetime.strptime(date_time_train_boundary, '%d.%m.%Y %H:%M:%S')\n",
        "\n",
        "# Convert date time string to Unix timestamp in seconds\n",
        "date_time_train_boundary = bytes(str(datetime.timestamp(date_time_train_boundary)), 'utf-8')\n",
        "\n",
        "\n",
        "def partition_fn(line, num_partitions):\n",
        "  '''\n",
        "  Partition function to work with Beam.partition\n",
        "\n",
        "  Args:\n",
        "    line (string) - One record in the CSV file.\n",
        "    num_partition (integer) - Number of partitions. Required argument by Beam. Unused in this function.\n",
        "\n",
        "  Returns:\n",
        "    0 or 1 (integer) - 0 if line timestamp is below the date time boundary, 1 otherwise. \n",
        "  '''\n",
        "\n",
        "  # Split the CSV string to a list\n",
        "  line_split = line.split(b',')\n",
        "\n",
        "  # Get the timestamp of the current line\n",
        "  line_dt = line_split[date_time_idx]\n",
        "\n",
        "  # Check if it is above or below the date time boundary\n",
        "  partition_num = int(line_dt > date_time_train_boundary)\n",
        "\n",
        "  return partition_num"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWDCARvqnYpz"
      },
      "source": [
        "### Declare Schema for Cleaned Data\n",
        "\n",
        "Just like in the previous labs with TFX, you will want to declare a schema to make sure that your data input is parsed correctly. You can do that with the cell below. Take note that this will be used later after the data cleaning step. Thus, you can expect that the date time feature is now in seconds and assigned to be a float feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NpXb7N_vWkB"
      },
      "source": [
        "# Declare feature spec\n",
        "RAW_DATA_FEATURE_SPEC = dict(\n",
        "    [(name, tf.io.FixedLenFeature([], tf.float32))\n",
        "     for name in TIMESTAMP_FEATURES] +\n",
        "    [(name, tf.io.FixedLenFeature([], tf.float32))\n",
        "     for name in NUMERIC_FEATURES]\n",
        ")\n",
        "\n",
        "# Create schema from feature spec\n",
        "RAW_DATA_SCHEMA = tft.tf_metadata.schema_utils.schema_from_feature_spec(RAW_DATA_FEATURE_SPEC)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79ezG-xidnAP"
      },
      "source": [
        "### Create the `tf.Transform` pipeline\n",
        "\n",
        "Now you can define the TF Transform pipeline. It will follow these major steps:\n",
        "\n",
        "1. Read in the data using the CSV reader\n",
        "1. Remove outliers and reformat timestamps using the `clean_fn`.\n",
        "1. Partition the dataset into train and test splits using the `beam.Partition` transform.\n",
        "1. Preprocess the data splits using the `preprocessing_fn`.\n",
        "1. Write the result as a `TFRecord` of `Example` protos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJ_eE8nntSSs"
      },
      "source": [
        "import shutil\n",
        "from tfx_bsl.coders.example_coder import RecordBatchToExamplesEncoder\n",
        "from tfx_bsl.public import tfxio\n",
        "\n",
        "# Directory names for the TF Transform outputs\n",
        "WORKING_DIR = 'transform_dir'\n",
        "TRANSFORM_TRAIN_FILENAME = 'transform_train'\n",
        "TRANSFORM_TEST_FILENAME = 'transform_test'\n",
        "TRANSFORM_TEMP_DIR = 'tft_temp'\n",
        "\n",
        "\n",
        "# The \"with\" block will create a pipeline, and run that pipeline at the exit\n",
        "#   of the block.\n",
        "def read_and_transform_data(working_dir):\n",
        "  '''\n",
        "  Reads a CSV File and preprocesses the data using TF Transform\n",
        "\n",
        "  Args:\n",
        "    working_dir (string) - directory to place TF Transform outputs\n",
        "  \n",
        "  Returns:\n",
        "    transform_fn - transformation graph\n",
        "    transformed_train_data - transformed training examples\n",
        "    transformed_test_data - transformed test examples\n",
        "    transformed_metadata - transform output metadata\n",
        "  '''\n",
        "\n",
        "  # Delete TF Transform if it already exists\n",
        "  if os.path.exists(working_dir):\n",
        "    shutil.rmtree(working_dir)\n",
        "\n",
        "  with beam.Pipeline() as pipeline:\n",
        "      with tft_beam.Context(temp_dir=os.path.join(working_dir, TRANSFORM_TEMP_DIR)):\n",
        "        \n",
        "        # Read the input CSV and clean the data\n",
        "        raw_data = (\n",
        "              pipeline\n",
        "              | 'ReadTrainData' >> beam.io.ReadFromText(INPUT_FILE, coder=beam.coders.BytesCoder(), skip_header_lines=1)\n",
        "              | 'CleanLines' >> beam.Map(clean_fn))\n",
        "\n",
        "        # Partition the dataset into train and test sets using the partition_fn defined earlier.    \n",
        "        raw_train_data, raw_test_data = (raw_data\n",
        "                                 | 'TrainTestSplit' >> beam.Partition(partition_fn, 2))\n",
        "        \n",
        "        # Create a TFXIO to read the data with the schema. You need\n",
        "        # to list all columns in order since the schema doesn't specify the\n",
        "        # order of columns in the csv.\n",
        "        csv_tfxio = tfxio.BeamRecordCsvTFXIO(\n",
        "              physical_format='text',\n",
        "              column_names=ordered_columns,\n",
        "              schema=RAW_DATA_SCHEMA)\n",
        "\n",
        "        # Parse the raw train data into inputs for TF Transform\n",
        "        raw_train_data = (raw_train_data \n",
        "                          | 'DecodeTrainData' >> csv_tfxio.BeamSource())\n",
        "        \n",
        "        # Get the raw data metadata\n",
        "        RAW_DATA_METADATA = csv_tfxio.TensorAdapterConfig()\n",
        "        \n",
        "        # Pair the train data with the metadata into a tuple\n",
        "        raw_train_dataset = (raw_train_data, RAW_DATA_METADATA)\n",
        "\n",
        "        # Training data transformation. The TFXIO (RecordBatch) output format\n",
        "        # is chosen for improved performance.\n",
        "        (transformed_train_data,transformed_metadata) , transform_fn = (\n",
        "        raw_train_dataset | tft_beam.AnalyzeAndTransformDataset(preprocessing_fn, output_record_batches=True))\n",
        "\n",
        "\n",
        "        # Parse the raw data into inputs for TF Transform\n",
        "        raw_test_data = (raw_test_data\n",
        "                         | 'DecodeTestData' >> csv_tfxio.BeamSource())\n",
        "        \n",
        "        # Pair the test data with the metadata into a tuple\n",
        "        raw_test_dataset = (raw_test_data, RAW_DATA_METADATA)\n",
        "        \n",
        "        # Now apply the same transform function to the test data.\n",
        "        # You don't need the transformed data schema. It's the same as before.\n",
        "        transformed_test_data, _ = (\n",
        "          (raw_test_dataset, transform_fn) | tft_beam.TransformDataset(output_record_batches=True))\n",
        "        \n",
        "        # Declare an encoder to convert output record batches to TF Examples \n",
        "        transformed_data_coder = RecordBatchToExamplesEncoder(transformed_metadata.schema)\n",
        "        \n",
        "        # Encode transformed train data and write to disk\n",
        "        _ = (\n",
        "            transformed_train_data\n",
        "            | 'EncodeTrainData' >> beam.FlatMapTuple(lambda batch, _: transformed_data_coder.encode(batch))\n",
        "            | 'WriteTrainData' >> beam.io.WriteToTFRecord(\n",
        "                os.path.join(working_dir, TRANSFORM_TRAIN_FILENAME)))\n",
        "\n",
        "        # Encode transformed test data and write to disk\n",
        "        _ = (\n",
        "            transformed_test_data\n",
        "            | 'EncodeTestData' >> beam.FlatMapTuple(lambda batch, _: transformed_data_coder.encode(batch))\n",
        "            | 'WriteTestData' >> beam.io.WriteToTFRecord(\n",
        "                os.path.join(working_dir, TRANSFORM_TEST_FILENAME)))\n",
        "        \n",
        "        # Write transform function to disk\n",
        "        _ = (\n",
        "          transform_fn\n",
        "          | 'WriteTransformFn' >>\n",
        "          tft_beam.WriteTransformFn(os.path.join(working_dir)))\n",
        "\n",
        "         \n",
        "  return transform_fn, transformed_train_data, transformed_test_data, transformed_metadata"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GPbh3qKzyTb"
      },
      "source": [
        "def main():\n",
        "  return read_and_transform_data(WORKING_DIR)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  transform_fn, transformed_train_data, trainsformed_test_data, transformed_metadata = main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CqjN6qrTzQ5"
      },
      "source": [
        "##Prepare Training and Test Datasets from TFTransformOutput\n",
        "\n",
        "Now that you have the transformed dataset, you will need to map it to training and test datasets whch can be used for training a model using TensorFlow. Since this is time series data, it makes sense to group a fixed-length series of measurements and map that to the label found in a future time step. For example, 3 days of data can be used to predict the next day's humidity. You can use the [tf.data.Dataset.window()](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#window) method to implement these groupings.\n",
        "\n",
        "In this exercise, you will use data from the last 5 days to predict the temperature 12 hours into the future. \n",
        "\n",
        "* You already know that data is taken every 10 minutes so there will be 720 data points for 5 days. \n",
        "* You will also assume that drastic change is not expected within an hour so you will downsample the 720 data points to 120 by just getting the data every hour. Thus, you will use a *stride* (or step size) of 6 when getting the data points. This makes the `HISTORY_SIZE` for the feature window equal to 120. \n",
        "* For this single step prediction model (forecasting), the label for a dataset window will be the temperature 12 hours into the future.\n",
        "* By default, the `window()` method moves `size` (i.e. window size) steps at a time. For example, if you have a dataset with elements `[1, 2, 3, 4]` and you have `size=2`, then your results will look like: `[1, 2], [3, 4]`. You can reconfigure this behavior and move at smaller or larger steps with the `shift` parameter. For the same sample dataset with window `size=2` but with `shift=1`, the results will look like `[1, 2], [2,3], [3,4]` because the window is moving `1` element at a time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho06omK_OAVH"
      },
      "source": [
        "# Constants to prepare the transformed data for modeling\n",
        "\n",
        "LABEL_KEY = 'T (degC)'\n",
        "OBSERVATIONS_PER_HOUR = 6\n",
        "HISTORY_SIZE = 120\n",
        "FUTURE_TARGET = 12\n",
        "BATCH_SIZE = 72\n",
        "SHIFT = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J8ljQwSQh8b"
      },
      "source": [
        "# Get the output of the Transform component\n",
        "tf_transform_output = tft.TFTransformOutput(os.path.join(WORKING_DIR))\n",
        "\n",
        "# Get the index of the label key\n",
        "index_of_label = list(tf_transform_output.transformed_feature_spec().keys()).index(LABEL_KEY)\n",
        "print(index_of_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_qnkrUJt9wO"
      },
      "source": [
        "Next, you will define several helper functions to extract the data from your transformed dataset and group it into windows. First, this `parse_function()` will help in getting the transformed features and rearranging them so that the label values (i.e. `T (degC)`) is at the end of the tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQxYFAXkNUwP"
      },
      "source": [
        "def parse_function(example_proto):\n",
        "    \n",
        "    feature_spec = tf_transform_output.transformed_feature_spec()\n",
        "    \n",
        "    # Define features with the example_proto (transformed data) and the feature_spec using tf.io.parse_single_example \n",
        "    features = tf.io.parse_single_example(example_proto, feature_spec)\n",
        "    values = list(features.values())\n",
        "    values[index_of_label], values[len(features) - 1] = values[len(features) - 1], values[index_of_label]\n",
        "    \n",
        "    # Stack the values along the first axis\n",
        "    stacked_features = tf.stack(values, axis=0)\n",
        "\n",
        "    return stacked_features\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNkCb1gkPQ0R"
      },
      "source": [
        "Next, you will separate the features and target values into a tuple with this utility function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3xcDvgIOrvl"
      },
      "source": [
        "def map_features_target(elements):\n",
        "    features = elements[:HISTORY_SIZE]\n",
        "    target = elements[-1:,-1]\n",
        "    return (features, target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abOkQXarPIYl"
      },
      "source": [
        "Finally, you can define the dataset window with the function below. It uses the parameters defined above and also the helper functions to produce the batched feature-target mappings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAUbyWGfPUha"
      },
      "source": [
        "def get_windowed_dataset(path):\n",
        "        \n",
        "    # Instantiate a tf.data.TFRecordDataset passing in the appropiate path\n",
        "    dataset = tf.data.TFRecordDataset(path)\n",
        "    \n",
        "    # Use the dataset's map method to map the parse_function\n",
        "    dataset = dataset.map(parse_function)\n",
        "    \n",
        "    # Use the window method with expected total size. Define stride and set drop_remainder to True\n",
        "    dataset = dataset.window(HISTORY_SIZE + FUTURE_TARGET, shift=SHIFT, stride=OBSERVATIONS_PER_HOUR, drop_remainder=True)\n",
        "    \n",
        "    # Use the flat_map method passing in an anonymous function that given a window returns window.batch(HISTORY_SIZE + FUTURE_TARGET)\n",
        "    dataset = dataset.flat_map(lambda window: window.batch(HISTORY_SIZE + FUTURE_TARGET))\n",
        "    \n",
        "    # Use the map method passing in the previously defined map_features_target function\n",
        "    dataset = dataset.map(map_features_target) \n",
        "    \n",
        "    # Use the batch method and pass in the appropiate batch size\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-Afrxg2QYeT"
      },
      "source": [
        "You can now use the `get_dataset()` function on your transformed examples to produce the dataset windows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0grFNuaTsk3"
      },
      "source": [
        "# Get list of train and test data tfrecord filenames from the transform outputs\n",
        "train_tfrecord_files = tf.io.gfile.glob(os.path.join(WORKING_DIR, TRANSFORM_TRAIN_FILENAME + '*'))\n",
        "test_tfrecord_files = tf.io.gfile.glob(os.path.join(WORKING_DIR, TRANSFORM_TEST_FILENAME + '*'))\n",
        "\n",
        "# Generate dataset windows\n",
        "windowed_train_dataset = get_windowed_dataset(train_tfrecord_files[0])\n",
        "windowed_test_dataset = get_windowed_dataset(test_tfrecord_files[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AmBzD-OQjMG"
      },
      "source": [
        "Let's preview the resulting shapes of the data windows. If you print the shapes of the tensors in a single batch, you'll notice that it indeed produced the required dimensions. It has 72 examples per batch where each contain 120 measurements for each of the 13 features in the transformed dataset. The target tensor shape only has one feature per example in the batch as expected (i.e. only `T (degC)`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgTJDfGpt8af"
      },
      "source": [
        "ordered_feature_spec_names = tf_transform_output.transformed_feature_spec().keys()\n",
        "\n",
        "# Preview an example in the train dataset\n",
        "for features, target  in windowed_train_dataset.take(1):\n",
        "    print(f'Shape of input features for a batch: {features.shape}')\n",
        "    print(f'Shape of targets for a batch: {target.shape}\\n')\n",
        "\n",
        "    print(f'INPUT FEATURES:')\n",
        "    for value, name in zip(features[0][0].numpy(), ordered_feature_spec_names):\n",
        "      print(f'{name} : {value}') \n",
        "  \n",
        "    print(f'\\nTARGET TEMPERATURE: {target[0][0]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60TEQ9tcMORG"
      },
      "source": [
        "# Preview an example in the test dataset\n",
        "for features, target in windowed_test_dataset.take(1):\n",
        "    print(f'Shape of input features for a batch: {features.shape}')\n",
        "    print(f'Shape of targets for a batch: {target.shape}\\n')\n",
        "\n",
        "    print(f'INPUT FEATURES:')\n",
        "    for value, name in zip(features[0][0].numpy(), ordered_feature_spec_names):\n",
        "      print(f'{name} : {value}') \n",
        "  \n",
        "    print(f'\\nTARGET TEMPERATURE: {target[0][0]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsqC0wqmk9h4"
      },
      "source": [
        "## Wrap Up\n",
        "\n",
        "In this notebook, you got to see how you may want to prepare seasonal data. It shows how you can handle periodicity and produce batches of dataset windows. You will be doing something similar in the next lab with sensor data. This time though, the measurements are taken at a much higher rate (20 Hz). The labels are also categorical so you will be handling that differently.\n",
        "\n",
        "On to the next!"
      ]
    }
  ]
}